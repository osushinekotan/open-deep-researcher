{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import dotenv\n",
    "\n",
    "from open_deep_researcher.retriever.local import local_search, process_documents, search_local_documents\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Document Directory\n",
    "\n",
    "First, let's set up a directory with some sample documents to process. You can customize this to use your own documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for sample documents\n",
    "doc_dir = Path(\"./tmp/sample_docs\")\n",
    "doc_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Create some sample documents\n",
    "sample_docs = [\n",
    "    (\n",
    "        \"ai_intro.txt\",\n",
    "        \"Artificial intelligence (AI) is intelligence demonstrated by machines. Modern AI systems can learn from data and make predictions.\",\n",
    "    ),\n",
    "    (\n",
    "        \"python_basics.md\",\n",
    "        \"# Python Basics\\n\\nPython is a high-level programming language. It's known for readability and simplicity.\",\n",
    "    ),\n",
    "    (\n",
    "        \"data_science.txt\",\n",
    "        \"Data science combines domain expertise, programming skills, and statistical knowledge to extract insights from data.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Write the sample documents to the directory\n",
    "for filename, content in sample_docs:\n",
    "    file_path = doc_dir / filename\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.write(content)\n",
    "    print(f\"Created: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Process Documents Parameters\n",
    "\n",
    "Now, let's set up the parameters for the `process_documents` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up vector store path\n",
    "vector_store_path = Path(\"./tmp/vector_store\")\n",
    "vector_store_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Define parameters for process_documents\n",
    "params = {\n",
    "    \"local_document_path\": str(doc_dir),\n",
    "    \"vector_store_path\": str(vector_store_path),\n",
    "    \"embedding_provider\": \"openai\",  # Currently only OpenAI embeddings are supported\n",
    "    \"embedding_model\": \"text-embedding-3-small\",\n",
    "    \"collection_name\": \"sample_collection\",  # Optional: custom collection name\n",
    "}\n",
    "\n",
    "# Display the parameters\n",
    "print(\"Processing documents with the following parameters:\")\n",
    "for key, value in params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Documents\n",
    "\n",
    "Now let's call the `process_documents` function to actually process the documents and create embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an async function to run the process_documents function\n",
    "async def run_process_documents():\n",
    "    print(\"Starting document processing...\")\n",
    "    vector_store = await process_documents(**params)\n",
    "    print(\"Document processing completed.\")\n",
    "    return vector_store\n",
    "\n",
    "\n",
    "# Run the async function\n",
    "vector_store = await run_process_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the Vector Store\n",
    "\n",
    "Let's look at what was created in the vector store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the files in the vector store directory\n",
    "print(\"Files in the vector store directory:\")\n",
    "for file_path in vector_store_path.glob(\"*\"):\n",
    "    print(f\"  {file_path.name}\")\n",
    "\n",
    "# Examine the metadata file if it exists\n",
    "metadata_path = vector_store_path / f\"doc_metadata_{params['collection_name']}.json\"\n",
    "if metadata_path.exists():\n",
    "    with open(metadata_path) as f:\n",
    "        metadata = json.load(f)\n",
    "    print(\"\\nDocument metadata:\")\n",
    "    for doc_path, doc_hash in metadata.items():\n",
    "        print(f\"  {doc_path}: {doc_hash}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Incremental Processing\n",
    "\n",
    "One of the key features of `process_documents` is that it tracks document changes and only reprocesses documents that have changed. Let's test this by modifying one document and adding a new one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify an existing document\n",
    "modified_file = doc_dir / \"ai_intro.txt\"\n",
    "original_content = modified_file.read_text()\n",
    "new_content = (\n",
    "    original_content\n",
    "    + \"\\n\\nAI systems are used in many applications including natural language processing, computer vision, and robotics.\"\n",
    ")\n",
    "modified_file.write_text(new_content)\n",
    "print(f\"Modified: {modified_file}\")\n",
    "\n",
    "\n",
    "# Add a new document\n",
    "new_file = doc_dir / \"machine_learning.txt\"\n",
    "new_file.write_text(\n",
    "    \"Machine learning is a subset of AI that enables systems to learn from data without being explicitly programmed.\"\n",
    ")\n",
    "print(f\"Created: {new_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run process_documents again\n",
    "print(\"Running process_documents again to process changed files...\")\n",
    "updated_vector_store = await run_process_documents()\n",
    "\n",
    "# Check the updated metadata\n",
    "if metadata_path.exists():\n",
    "    with open(metadata_path) as f:\n",
    "        updated_metadata = json.load(f)\n",
    "    print(\"\\nUpdated document metadata:\")\n",
    "    for doc_path, doc_hash in updated_metadata.items():\n",
    "        print(f\"  {doc_path}: {doc_hash}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with the Vector Store\n",
    "\n",
    "The `process_documents` function returns a Chroma vector store instance. We can use this directly to perform operations on the vector store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have a valid vector store\n",
    "if updated_vector_store is not None:\n",
    "    # Get the vector store collection\n",
    "    collection = updated_vector_store._collection\n",
    "\n",
    "    # Get the count of documents in the vector store\n",
    "    count = collection.count()\n",
    "    print(f\"Number of document chunks in vector store: {count}\")\n",
    "\n",
    "    # Get some information about the collection\n",
    "    print(f\"Collection name: {collection.name}\")\n",
    "else:\n",
    "    print(\"Vector store not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Query Example\n",
    "\n",
    "Let's try a simple query against the vector store to verify that it's working.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a simple similarity search\n",
    "if updated_vector_store is not None:\n",
    "    query = \"What is artificial intelligence?\"\n",
    "    results = updated_vector_store.similarity_search_with_relevance_scores(query, k=4)\n",
    "\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(\"Results:\")\n",
    "    for i, (doc, score) in enumerate(results, 1):\n",
    "        print(f\"\\nResult {i} (Score: {score:.4f})\")\n",
    "        print(f\"Content: {doc.page_content}\")\n",
    "        print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an async function to run local_search\n",
    "async def run_search_local_docments(query, vector_store_path, collection_name=\"sample_collection\", top_k=2):\n",
    "    results = await search_local_documents(\n",
    "        query=query, vector_store_path=str(vector_store_path), collection_name=collection_name, top_k=top_k\n",
    "    )\n",
    "    return results\n",
    "\n",
    "\n",
    "async def run_local_search(query, vector_store_path, collection_name=\"sample_collection\", top_k=2):\n",
    "    queries = [query]  # local_search expects a list of queries\n",
    "    results = await local_search(\n",
    "        search_queries=queries, vector_store_path=str(vector_store_path), collection_name=collection_name, top_k=top_k\n",
    "    )\n",
    "    return results\n",
    "\n",
    "\n",
    "# Define a test query\n",
    "test_query = \"What is artificial intelligence?\"\n",
    "print(f\"Query: '{test_query}'\")\n",
    "\n",
    "\n",
    "# Define a test query\n",
    "test_query = \"What is artificial intelligence?\"\n",
    "print(f\"Query: '{test_query}'\")\n",
    "\n",
    "# Get results using local_search function\n",
    "print(\"\\n=== RESULTS USING search_local_documents FUNCTION ===\")\n",
    "local_search_results = await run_search_local_docments(test_query, vector_store_path)\n",
    "for i, doc in enumerate(local_search_results[0][\"results\"], 1):\n",
    "    print(f\"\\nResult {i} (Score: {doc['score']:.4f})\")\n",
    "    print(f\"Content: {doc['content']}\")\n",
    "    print(f\"Source: {doc.get('url', 'Unknown')}\")\n",
    "\n",
    "# Get results using direct Chroma similarity search\n",
    "print(\"\\n=== RESULTS USING DIRECT CHROMA QUERY ===\")\n",
    "if updated_vector_store is not None:\n",
    "    chroma_results = updated_vector_store.similarity_search_with_relevance_scores(test_query, k=2)\n",
    "\n",
    "    for i, (doc, score) in enumerate(chroma_results, 1):\n",
    "        print(f\"\\nResult {i} (Score: {score:.4f})\")\n",
    "        print(f\"Content: {doc.page_content}\")\n",
    "        print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "\n",
    "# Get results using local_search function\n",
    "print(\"\\n=== RESULTS USING local_search FUNCTION ===\")\n",
    "local_search_results = await run_local_search(test_query, vector_store_path)\n",
    "print(local_search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)\n",
    "\n",
    "If you want to clean up the directories created in this notebook, you can run the following cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment these lines if you want to clean up the directories\n",
    "import shutil\n",
    "\n",
    "shutil.rmtree(\"./tmp\", ignore_errors=True)\n",
    "print(\"Cleaned up sample directories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
